{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from sklearn.utils import Bunch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import spacy\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import pandas as pd\n",
    "import requests\n",
    "import psycopg2\n",
    "import pandas\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from sqlalchemy import Column, Date, Integer, String, TIMESTAMP\n",
    "from sqlalchemy.orm import sessionmaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:azerty09@localhost:5432/netflix')\n",
    "Base = declarative_base()\n",
    "topic_name = 'netflix'\n",
    "API_ENDPOINT = 'https://api.powerbi.com/beta/901cb4ca-b862-4029-9306-e5cd0f6d9f86/datasets/e33ab758-a149-4bf6-848c-9c3b73043548/rows?key=8%2B3ooFppRM%2Fthmd3ozdmaUyZpVhLt%2BhwaFgvK8QWV2eCbW%2Fry6GYkMDiAM%2Bke6m9Dp%2BvA0HJi9hsjncCQY%2BmMw%3D%3D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "     topic_name,\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='latest',\n",
    "     enable_auto_commit=True,\n",
    "     auto_commit_interval_ms =  5000,\n",
    "     fetch_max_bytes = 1024,\n",
    "     max_poll_records = 500,\n",
    "\n",
    "     value_deserializer=lambda x: json.loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(data):\n",
    "    lemma_text = []\n",
    "    for text in data:\n",
    "        nlp_text = nlp(text)\n",
    "        buffer = []\n",
    "        for token in nlp_text:\n",
    "            lemma_token = token.lemma_\n",
    "            buffer.append(lemma_token)\n",
    "        lemma_text.append(buffer)\n",
    "    filtered_text = []\n",
    "    for text in lemma_text:\n",
    "        buffer = []\n",
    "        for token in text:\n",
    "            term = nlp.vocab[token]\n",
    "            if term.is_stop == False:\n",
    "                buffer.append(token)\n",
    "        filtered_text.append(buffer)\n",
    "    refiltered_text = []\n",
    "    for text in filtered_text:\n",
    "        buffer = []\n",
    "        for token in text:\n",
    "            if re.sub(r\"[^a-zA-ZÀ-ÿ]+\", ' ', token) != ' ':\n",
    "                buffer.append(re.sub(r\"[^a-zA-ZÀ-ÿ]+\", '', token))\n",
    "        refiltered_text.append(buffer)\n",
    "    my_stopwords = ['PRON', 'Twitter', 'RT']\n",
    "    second_filter = []\n",
    "    for text in refiltered_text:\n",
    "        for token in text:\n",
    "            if token not in my_stopwords and token.find('http') == -1 and len(token) > 2:\n",
    "                second_filter.append(token)\n",
    "    return second_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as handle:\n",
    "    model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tweet_lemma, serie):\n",
    "  \n",
    "    #X_test = []\n",
    "    tweet_lemma = str(tweet_lemma).strip('[]')\n",
    "    #string = str(tweet_lemma).strip('[]')   \n",
    "    #X_test.append(re.sub(\"'\", '', string))\n",
    "    tweet_lemma = re.sub(\"'\", '', tweet_lemma)\n",
    "    X_test = [tweet_lemma]\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if y_pred[0] == 0:\n",
    "        sentiment = 'negative'\n",
    "        timestamp = time.asctime()\n",
    "    else:\n",
    "        sentiment = 'positive'\n",
    "        timestamp = time.asctime()\n",
    "    \n",
    "#     dtime = 'Fri Oct 09 10:01:41 +0000 2015'\n",
    "#     new_datetime = datetime.strftime(datetime.strptime(dtime,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "#     print((new_datetime))\n",
    "    timestamp = datetime.strftime(datetime.strptime(timestamp,'%a %b %d %H:%M:%S %Y'), '%Y-%m-%d %H:%M:%S')\n",
    "    tweet = Tweet(tweet_lemma, sentiment, serie, timestamp)\n",
    "#     data = {'timestamp': timestamp, 'tweet': X_test, 'sentiment': sentiment, 'serie': serie}\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(Base):\n",
    "    \"\"\"\"\"\"\n",
    "    __tablename__ = \"test\"\n",
    " \n",
    "    id = Column(String, primary_key=True)\n",
    "    tweet = Column(String)\n",
    "    sentiment = Column(String)\n",
    "    serie = Column(String)\n",
    "    timestamp = Column(TIMESTAMP)\n",
    "    \n",
    "    #----------------------------------------------------------------------\n",
    "    def __init__(self, tweet, sentiment, serie, timestamp):\n",
    "        \"\"\"\"\"\"\n",
    "        self.id = uuid.uuid4()\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.serie = serie\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "# create tables\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lupin, great, omarsy, project, netflix  -  positive  -  Lupin  -  2021-01-15 19:12:12  -  7f146ba6-44e9-4f51-990f-070024c8c4b4\n",
      "wait, Lupin, movie, made, actually, movie, want, watch, binge  -  positive  -  Lupin  -  2021-01-15 19:12:42  -  2e3a0ece-2d79-4d37-9fad-3c6aa85c3062\n",
      "lupin, good, series  -  positive  -  Lupin  -  2021-01-15 19:12:52  -  b366a5cd-0d35-478b-8e88-513893150ffa\n",
      "lupin, great, omarsy, project, netflix  -  positive  -  Lupin  -  2021-01-15 19:12:58  -  3d9676d7-2f28-44a5-993f-048705e6caa5\n",
      "gyxnx, peaky, blinder, paia  -  positive  -  Peaky Blinders  -  2021-01-15 19:13:10  -  f7b2619c-7fbb-4316-aaa6-c1eff4df7bf9\n",
      "EPM, CGI, Lupin, movie  -  positive  -  Lupin  -  2021-01-15 19:13:17  -  c95199c7-5c3c-480d-8a36-6560c1898f33\n",
      "LukeTurnerEsq, Queens, Gambit, Young, Wallander, Serpent, undoing, Lupin  -  positive  -  Lupin  -  2021-01-15 19:13:24  -  69bc964b-5fea-47c9-97f8-1b6251f39925\n",
      "start, Lupin  -  positive  -  Lupin  -  2021-01-15 19:13:34  -  1efca406-6db0-40b8-9b41-40e38c6c59b5\n",
      "Tommy, shovel, shit, curly, like, Tommy, remind, meself, Peaky, Blinders, series  -  positive  -  Peaky Blinders  -  2021-01-15 19:13:35  -  529c85c7-9885-4f0f-bd40-0f677e62cc60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a7d39a076f84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_v1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py\u001b[0m in \u001b[0;36mnext_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_message_generator_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py\u001b[0m in \u001b[0;36m_message_generator_v2\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_message_generator_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consumer_timeout\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m         \u001b[0mrecord_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1117\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m             \u001b[1;31m# Generators are stateful, and it is possible that the tp / records\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m             \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll_once\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_records\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate_offsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\consumer\\group.py\u001b[0m in \u001b[0;36m_poll_once\u001b[1;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_to_next_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m         \u001b[1;31m# after the long poll, we should check whether the group needs to rebalance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m         \u001b[1;31m# prior to returning data so that the group can stabilize faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\client_async.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout_ms, future)\u001b[0m\n\u001b[0;32m    600\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# avoid negative timeouts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m             \u001b[1;31m# called without the lock to avoid deadlock potential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\site-packages\\kafka\\client_async.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[0mstart_select\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[0mend_select\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\selectors.py\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python39\\lib\\selectors.py\u001b[0m in \u001b[0;36m_select\u001b[1;34m(self, r, w, _, timeout)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'win32'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create a configured \"Session\" class\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "# create a Session\n",
    "session = Session()\n",
    "\n",
    "for message in consumer:\n",
    "    tweet = json.loads(json.dumps(message.value))\n",
    "    data = None\n",
    "    if 'retweeted_status' in tweet:\n",
    "        None\n",
    "    elif 'extended_tweet' in tweet:\n",
    "        if 'lupin' in tweet['extended_tweet']['full_text'].lower():\n",
    "            tweet_lemma = cleanText([tweet['extended_tweet']['full_text']])\n",
    "            \n",
    "            data = prediction(tweet_lemma, 'Lupin')\n",
    "        \n",
    "        if 'peaky' in tweet['extended_tweet']['full_text'].lower():\n",
    "            tweet_lemma = cleanText([tweet['extended_tweet']['full_text']])\n",
    "            \n",
    "            data = prediction(tweet_lemma, 'Peaky Blinders')\n",
    "            \n",
    "    else:\n",
    "        if 'lupin' in tweet['text'].lower():\n",
    "            tweet_lemma = cleanText([tweet['text']])\n",
    "            \n",
    "            data = prediction(tweet_lemma, 'Lupin')\n",
    "        \n",
    "        if 'peaky blinders' in tweet['text'].lower():\n",
    "            tweet_lemma = cleanText([tweet['text']])\n",
    "            \n",
    "            data = prediction(tweet_lemma, 'Peaky Blinders')\n",
    "    \n",
    "    \n",
    "    # add data to postgreSQL\n",
    "    if data != None:\n",
    "        print(data.tweet, ' - ', data.sentiment, ' - ', data.serie, ' - ', data.timestamp, ' - ', data.id)\n",
    "        session.add(data)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Récupération des tweets dans un dataframe + count len (=nombre de tweets) et envoi sur l'API PowerBI\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# tweets = pd.DataFrame()\n",
    "# cpt = 0\n",
    "# for message in consumer:\n",
    "#     tweet_info_to_keep_dict = {}\n",
    "#     current_tweets_dict = message.value\n",
    "#     tweet_info_to_keep_dict.update({'tweet_content': current_tweets_dict[\"text\"]})\n",
    "#     tweets = tweets.append(tweet_info_to_keep_dict, ignore_index=True)\n",
    "#     display(tweets)\n",
    "#     tweet_count = len(tweets.index)\n",
    "    \n",
    "#     tweet_count_df = pd.DataFrame()\n",
    "#     tweet_count_df = tweet_count_df.append({\"tweet_count\": tweet_count}, ignore_index=True)\n",
    "#     print(tweet_count_df)\n",
    "#     data = bytes(tweet_count_df.to_json(orient = 'records'), encoding='utf-8')\n",
    "#     print(data)\n",
    "#     req = requests.post(API_ENDPOINT, data)\n",
    "#     print(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for message in consumer:\n",
    "#     tweets = json.loads(json.dumps(message.value))\n",
    "#     print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connection sur postgresql et envoi de la data dans une tab\n",
    "\n",
    "# import psycopg2\n",
    "# import pandas\n",
    "# import sqlalchemy\n",
    " \n",
    "# # Create the engine to connect to the PostgreSQL database\n",
    "# engine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:azerty09@localhost:5432/netflix')\n",
    " \n",
    "# # Read data from CSV and load into a dataframe object\n",
    "# data = pandas.read_csv(r'C:\\Users\\tadav\\OneDrive\\Bureau\\Data&GO\\PROJET 3 TWITTER\\superstore.csv')\n",
    " \n",
    "# # Write data into the table in PostgreSQL database\n",
    "# data.to_sql('test',engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
